package chat

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"strings"

	httperrors "github.com/portainer/portainer-ee/api/http/errors"
	"github.com/portainer/portainer-ee/api/http/security"
	str "github.com/portainer/portainer-ee/api/internal/string"
	"github.com/portainer/portainer-ee/api/openai"
	portainer "github.com/portainer/portainer/api"
	httperror "github.com/portainer/portainer/pkg/libhttp/error"
	"github.com/portainer/portainer/pkg/libhttp/request"
	"github.com/portainer/portainer/pkg/libhttp/response"

	"github.com/asaskevich/govalidator"
	"github.com/rs/zerolog/log"
)

type (
	chatQueryPayload struct {
		// Message is the message that will be forwarded to the OpenAI API.
		Message string `validate:"required" example:"I would like to deploy a wordpress production setup"`
		// Context is used to build a context that will be added to the prompt sent to OpenAI.
		// Only accepts a list of specific values:
		// environment_aware will create a context based on environment information (EnvironmentID parameter is mandatory).
		Context string `validate:"required" example:"environment_aware"`
		// EnvironmentID is the ID of the environment for which the chat query is being made
		// it will be used to retrieve context information about the environment and pass it to then OpenAI API.
		// Mandatory when environment_aware context is used.
		EnvironmentID int `example:"1"`
		// Model is the name of the OpenAI model that will be used to generate the response.
		// Only accepts a list of specific values: text-davinci-003, gpt-3.5-turbo, gpt-4
		// Note that the use of the GPT-4 model requires access to its API.
		// Optional: will default to gpt-3.5-turbo if not provided.
		Model string `example:"gpt-3.5-turbo"`
	}

	chatQueryResponse struct {
		// Message is the message that is returned by the OpenAI API.
		Message string `json:"message"`
		// Yaml is the content of the compose / kubernetes manifest file that was generated by the OpenAI API as part of the response.
		Yaml string `json:"yaml"`
	}
)

func (payload *chatQueryPayload) Validate(r *http.Request) error {
	if govalidator.IsNull(payload.Message) {
		return errors.New("missing mandatory payload parameter: message")
	}
	if govalidator.IsNull(payload.Context) {
		return errors.New("missing mandatory payload parameter: context")
	}
	if !govalidator.IsIn(payload.Context, string(openai.PROMPT_ENVIRONMENT_AWARE)) {
		return fmt.Errorf("invalid context parameter. Supported values: %s", strings.Join([]string{string(openai.PROMPT_ENVIRONMENT_AWARE)}, ", "))
	}
	if payload.Context == string(openai.PROMPT_ENVIRONMENT_AWARE) && payload.EnvironmentID == 0 {
		return fmt.Errorf("missing payload parameter: environmentID. Parameter is mandatory when context is set to %s", string(openai.PROMPT_ENVIRONMENT_AWARE))
	}
	if !govalidator.IsNull(payload.Model) && !govalidator.IsIn(payload.Model, openai.SupportedModelList()...) {
		return fmt.Errorf("invalid model parameter. Supported values: %s", strings.Join(openai.SupportedModelList(), ", "))
	}
	if govalidator.IsNull(payload.Model) {
		payload.Model = string(openai.MODEL_GPT_3_5_TURBO)
	}
	return nil
}

// @id chatQuery
// @summary Send a chat query to the OpenAI API
// @description Send a chat query to the OpenAI API.
// @description **Access policy**: user
// @tags chat
// @security ApiKeyAuth
// @security jwt
// @accept json
// @produce json
// @param body body chatQueryPayload true "Query"
// @success 200 {object} chatQueryResponse "Success"
// @failure 500 "Server error"
// @router /chat [post]
func (handler *Handler) chatQuery(w http.ResponseWriter, r *http.Request) *httperror.HandlerError {
	var payload chatQueryPayload
	err := request.DecodeAndValidateJSONPayload(r, &payload)
	if err != nil {
		return httperror.BadRequest("Invalid request payload", err)
	}

	settings, err := handler.DataStore.Settings().Settings()
	if err != nil {
		return httperror.InternalServerError("Unable to retrieve settings from the database", err)
	}

	if !settings.ExperimentalFeatures.OpenAIIntegration {
		return httperror.Forbidden("OpenAI integration is not enabled", httperrors.ErrNotAvailable)
	}

	tokenData, err := security.RetrieveTokenData(r)
	if err != nil {
		return httperror.InternalServerError("Unable to retrieve user authentication token", err)
	}

	user, err := handler.DataStore.User().Read(tokenData.ID)
	if err != nil {
		return httperror.NotFound("Unable to retrieve user from the database", err)
	}

	if user.OpenAIApiKey == "" {
		return httperror.NewError(http.StatusServiceUnavailable, "Unable to use OpenAI chat", errors.New("OpenAI API key not set"))
	}

	contextBuilder := openai.NewPromptBuilder(handler.DataStore, handler.SnapshotService)

	promptParameters := openai.PromptParameters{
		PromptType:    openai.PromptType(payload.Context),
		UserMessage:   payload.Message,
		EnvironmentID: portainer.EndpointID(payload.EnvironmentID),
		User:          *user,
	}

	flagged, err := openai.SendModerationRequest(r.Context(), promptParameters.UserMessage, user.OpenAIApiKey)
	if err != nil {
		log.Err(err).Msg("unable to query OpenAI for moderation")
		return httperror.InternalServerError("An error occured while querying OpenAI moderation API", err)
	}

	if flagged {
		return httperror.BadRequest("Message flagged by OpenAI", errors.New("the message has been flagged as not compliant with the OpenAI usage policies"))
	}

	openAIResponse, err := sendOpenAIRequest(r.Context(), contextBuilder, promptParameters, user.OpenAIApiKey, payload.Model)
	if err != nil {
		log.Err(err).Msg("unable to query OpenAI")
		return httperror.InternalServerError("An error occured while querying OpenAI", err)
	}

	chatResponse := chatQueryResponse{
		Message: openAIResponse,
	}

	yaml, found := str.GetStringInBetweenTwoString(openAIResponse, "```", "```")
	if found {
		chatResponse.Yaml = strings.TrimPrefix(yaml, "yaml")
	}

	return response.JSON(w, chatResponse)
}

// send API request to OpenAI based on selected model
// See https://platform.openai.com/docs/models/model-endpoint-compatibility for more info about model compatibility
// On average, a query using the text-davinci-003 model takes approximately 10 seconds to complete.
// On average, a query using the gpt-3-5-turbo model takes approximately 35 second to complete.
func sendOpenAIRequest(ctx context.Context, contextBuilder openai.OpenAIPromptBuilder, params openai.PromptParameters, apiKey string, model string) (string, error) {
	switch model {
	case string(openai.MODEL_TEXT_DAVINCI_003):
		prompt, err := contextBuilder.BuildTextCompletionPrompt(params)
		if err != nil {
			log.Err(err).Msg("unable to build OpenAI completion prompt")
			return "", err
		}

		return openai.SendTextCompletionRequest(ctx, model, prompt, apiKey)
	case string(openai.MODEL_GPT_3_5_TURBO), string(openai.MODEL_GPT_4):
		prompt, err := contextBuilder.BuildChatCompletionPrompt(params)
		if err != nil {
			log.Err(err).Msg("unable to build OpenAI chat completion prompt")
			return "", err
		}

		return openai.SendChatCompletionRequest(ctx, model, prompt, apiKey)
	}

	return "", errors.New("unsupported model")
}
